{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/9 | Loss: 0.0000 | Episodes: 132 | Win count: 0 | Win rate: 0.000 | Time: 26.5 seconds\n",
      "Epoch: 001/9 | Loss: 0.0001 | Episodes: 134 | Win count: 0 | Win rate: 0.000 | Time: 59.3 seconds\n",
      "Epoch: 002/9 | Loss: 0.0007 | Episodes: 139 | Win count: 0 | Win rate: 0.000 | Time: 94.8 seconds\n",
      "Epoch: 003/9 | Loss: 0.0007 | Episodes: 138 | Win count: 0 | Win rate: 0.000 | Time: 130.4 seconds\n",
      "Epoch: 004/9 | Loss: 0.0010 | Episodes: 64 | Win count: 1 | Win rate: 0.000 | Time: 147.0 seconds\n",
      "Epoch: 005/9 | Loss: 0.0013 | Episodes: 138 | Win count: 1 | Win rate: 0.000 | Time: 181.5 seconds\n",
      "Epoch: 006/9 | Loss: 0.0014 | Episodes: 150 | Win count: 1 | Win rate: 0.000 | Time: 219.7 seconds\n",
      "Epoch: 007/9 | Loss: 0.0004 | Episodes: 133 | Win count: 1 | Win rate: 0.000 | Time: 254.5 seconds\n",
      "Epoch: 008/9 | Loss: 0.0011 | Episodes: 94 | Win count: 2 | Win rate: 0.000 | Time: 280.0 seconds\n",
      "Epoch: 009/9 | Loss: 0.0014 | Episodes: 150 | Win count: 2 | Win rate: 0.000 | Time: 320.1 seconds\n",
      "n_epoch: 9, max_mem: 512, data: 32, time: 320.1 seconds\n",
      "Pirate got lost!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "# Define constants for actions\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "visited_mark = 0.8  # The visited cells are marked by an 80% gray shade.\n",
    "pirate_mark = 0.5   # The current cell where the pirate is located is marked by a 50% gray shade.\n",
    "\n",
    "class TreasureMaze(object):\n",
    "    def __init__(self, maze, pirate=(0, 0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows - 1, ncols - 1)   # target cell where the \"treasure\" is\n",
    "        self.free_cells = [(r, c) for r in range(nrows) for c in range(ncols) if self._maze[r, c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not pirate in self.free_cells:\n",
    "            raise Exception(\"Invalid Pirate Location: must sit on a free cell\")\n",
    "        self.reset(pirate)\n",
    "\n",
    "    def reset(self, pirate):\n",
    "        self.pirate = pirate\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = pirate\n",
    "        self.maze[row, col] = pirate_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = pirate_row, pirate_col, mode = self.state\n",
    "\n",
    "        if self.maze[pirate_row, pirate_col] > 0.0:\n",
    "            self.visited.add((pirate_row, pirate_col))\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:\n",
    "            mode = 'invalid'\n",
    "\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        pirate_row, pirate_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if pirate_row == nrows - 1 and pirate_col == ncols - 1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (pirate_row, pirate_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r, c] > 0.0:\n",
    "                    canvas[r, c] = 1.0\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = pirate_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        pirate_row, pirate_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if pirate_row == nrows - 1 and pirate_col == ncols - 1:\n",
    "            return 'win'\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows - 1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols - 1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row > 0 and self.maze[row - 1, col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row < nrows - 1 and self.maze[row + 1, col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col > 0 and self.maze[row, col - 1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col < ncols - 1 and self.maze[row, col + 1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions\n",
    "\n",
    "class GameExperience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            targets[i] = self.predict(envstate)\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)\n",
    "\n",
    "\n",
    "# Define a function to build the neural network model\n",
    "def build_model(maze):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "def completion_check(model, qmaze):\n",
    "    # Get the current state of the agent in the maze\n",
    "    current_state = qmaze.observe()\n",
    "    \n",
    "    # Predict the Q-values for the current state using the model\n",
    "    q_values = model.predict(current_state)\n",
    "    \n",
    "    # Get the action with the highest Q-value\n",
    "    action = np.argmax(q_values)\n",
    "    \n",
    "    # Check if the action leads the agent to the goal cell\n",
    "    if action in qmaze.valid_actions():\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "def play_game(model, qmaze, start):\n",
    "    qmaze.reset(start)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            print(\"Pirate found the treasure!\")\n",
    "            break\n",
    "        elif game_status == 'lose':\n",
    "            print(\"Pirate got lost!\")\n",
    "            break\n",
    "\n",
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    \n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    qmaze = TreasureMaze(maze)\n",
    "    experience = GameExperience(model, max_memory=max_memory)\n",
    "    \n",
    "    win_history = []\n",
    "    hsize = qmaze.maze.size // 2\n",
    "    win_rate = 0.0\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        agent_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(agent_cell)\n",
    "        envstate = qmaze.observe()\n",
    "        n_episodes = 0\n",
    "        \n",
    "        while True:\n",
    "            prev_envstate = envstate\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(0, num_actions)\n",
    "            else:\n",
    "                q = model.predict(prev_envstate)\n",
    "                action = np.argmax(q[0])\n",
    "            \n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_status]\n",
    "            experience.remember(episode)\n",
    "            \n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            loss = model.train_on_batch(inputs, targets)\n",
    "            \n",
    "            n_episodes += 1\n",
    "            \n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                break\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                break\n",
    "        \n",
    "        if epoch % hsize == 0:\n",
    "            if len(win_history) > hsize and np.mean(win_history[-hsize:]) > 0.9:\n",
    "                epsilon = 0.05\n",
    "        \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | Time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        \n",
    "        if sum(win_history[-hsize:]) == hsize:\n",
    "            if completion_check(model, qmaze):\n",
    "                print(\"Reached 100%% win rate at epoch: %d\" % int(epoch))\n",
    "                break\n",
    "    \n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    \n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# Set exploration factor\n",
    "epsilon = 0.1\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Define your maze as a 2D numpy array\n",
    "maze = np.array([\n",
    "    [1., 0., 1., 1., 1., 1., 1., 1.],\n",
    "    [1., 0., 1., 1., 1., 0., 1., 1.],\n",
    "    [1., 1., 1., 1., 0., 1., 0., 1.],\n",
    "    [1., 1., 1., 0., 1., 1., 1., 1.],\n",
    "    [1., 1., 0., 1., 1., 1., 1., 1.],\n",
    "    [1., 1., 1., 0., 1., 0., 0., 0.],\n",
    "    [1., 1., 1., 0., 1., 1., 1., 1.],\n",
    "    [1., 1., 1., 1., 0., 1., 1., 1.]\n",
    "])\n",
    "\n",
    "# Create and build the neural network model\n",
    "model = build_model(maze)\n",
    "# Define the starting position for the pirate\n",
    "pirate_start = (0, 0)\n",
    "# Train the model using deep Q-learning\n",
    "qtrain(model, maze, n_epoch=10, max_memory=8*maze.size, data_size=32)\n",
    "# Create the TreasureMaze environment\n",
    "qmaze = TreasureMaze(maze, pirate_start)\n",
    "# Test the model for one game\n",
    "pirate_start = (0, 0)\n",
    "play_game(model, qmaze, pirate_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
